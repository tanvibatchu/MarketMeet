{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the libraries you can use.  You may add any libraries directy related to threading if this is a direction\n",
    "#you wish to go (this is not from the course, so it's entirely on you if you wish to use threading).  Any\n",
    "#further libraries you wish to use you must email me, james@uwaterloo.ca, for permission.\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_financial as npf\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Assignment\n",
    "### Team Number: 10\n",
    "### Team Member Names: David, Tanvi, Johan\n",
    "### Team Strategy Chosen: Market Meet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contribution Declaration\n",
    "\n",
    "The following team members made a meaningful contribution to this assignment:\n",
    "\n",
    "David, Tanvi, and Johan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Markdown Introduction:\n",
    "\n",
    "Goal: Try to have the 25-stock portfolio’s returns match the average of the TSX composite + S&P 500’s return (~0.370%) as accurately as possible, over the 5-day period (Nov. 24 to Nov. 28).\n",
    "\n",
    "- **S&P/TSX Composite Index** (`^GSPTSE`)\n",
    "- **S&P 500 Index** (`^GSPC`)\n",
    "\n",
    "| Index | YTD Return | Daily Return (÷ 252)| 5-Day Estimated Return |\n",
    "|:------|:-----------:|:--------------------:|:----------------------:|\n",
    "| TSX Composite | 21.74% | 0.0863% | 0.43% |\n",
    "| S&P 500 | 15.55% | 0.0617% | 0.31% |\n",
    "| **Average (50/50)** | — | — | **≈ 0.370%** |\n",
    "\n",
    "Use historical data from `yfinance` to calculate:\n",
    "  - Mean returns\n",
    "  - Volatility (standard deviation)\n",
    "  - Beta and alpha values\n",
    "  - Correlation with the benchmark\n",
    "  - Idiosyncratic (residual) risk\n",
    "\n",
    "Select 10–25 stocks (we aim for 25 if possible) that:\n",
    "- Have **beta ≈ 1** and high correlation with the benchmark\n",
    "- Are liquid (average daily volume ≥ 5,000 shares)\n",
    "- Have sector diversification (no sector >40% of total value)\n",
    "- Include at least one large-cap (> 10B CAD) and one small-cap (< $2B CAD)\n",
    "\n",
    "Weights are between (100/(2n))% and 15% (for 25 stocks, between 2% and 15%)\n",
    "- Spend approximately **$1,000,000 CAD**, net of trading fees:\n",
    "- Fees = min(2.15 USD, 0.001 USD * shares) per trade, applied to all stock purchases\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## === Load Stock Tickers from the TA's CSV File ===\n",
    "\n",
    "tickers_df = pd.read_csv(\"Tickers_Example.csv\", header=None)\n",
    "tickers_list = tickers_df[0].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting synchronous download of price/volume data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5 Failed downloads:\n",
      "['MON', 'AGN', 'RTN', 'CELG']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "['^GSPTSE']: Timeout('Failed to perform, curl: (28) Operation timed out after 10002 milliseconds with 0 bytes received. See https://curl.se/libcurl/c/libcurl-errors.html first for more details.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price/Volume download complete.\n",
      "Starting asynchronous metadata fetching with delay...\n",
      "Metadata retrieval complete.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of ['Ticker'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2x/vtsxhcr16tn0q3f7vqmnw2nw0000gn/T/ipykernel_39208/2050186638.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mmetadata_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfetch_all_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Convert results to DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ticker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Re-align index order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Small-cap\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Small-cap\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Large-cap\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Large-cap\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   6118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6119\u001b[0m                         \u001b[0mmissing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\u001b[0m\u001b[0;34mNone of \u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m are in the columns\u001b[0m\u001b[0;34m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6125\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['Ticker'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# === 1. Original Setup & Data Download ===\n",
    "\n",
    "sp500 = \"^GSPC\"\n",
    "tsx = \"^GSPTSE\"\n",
    "start = \"2024-01-01\"\n",
    "end = \"2025-9-30\"\n",
    "\n",
    "min_vol = 5_000\n",
    "corr_min  = 0.20\n",
    "small_cap = 2_000_000_000\n",
    "large_cap = 10_000_000_000\n",
    "earn_low  = datetime(2025, 11, 24).date()\n",
    "earn_high = datetime(2025, 11, 28).date()\n",
    "\n",
    "# Download price + volume data\n",
    "tickers_all = tickers_list + [sp500, tsx]\n",
    "print(\"Starting synchronous download of price/volume data...\")\n",
    "data = yf.download(tickers_all, start=start, end=end, auto_adjust=False, progress=False)\n",
    "close = data[\"Close\"]\n",
    "vol = data[\"Volume\"]\n",
    "print(\"Price/Volume download complete.\")\n",
    "\n",
    "# Benchmark calculation\n",
    "bench = ((close[sp500] + close[tsx]) / 2).dropna()\n",
    "bench_ret = bench.pct_change().dropna()\n",
    "stocks = close[tickers_list].loc[bench_ret.index].dropna(how=\"all\", axis=1)\n",
    "rets = stocks.pct_change().dropna()\n",
    "\n",
    "# FX rate (Keep synchronous as it's a single, fast call)\n",
    "fx_raw = yf.Ticker(\"CADUSD=X\").history(period=\"1d\")[\"Close\"]\n",
    "usd_to_cad = fx_raw.iloc[-1] if len(fx_raw) > 0 else 0.73  # fallback rate\n",
    "\n",
    "async def async_get_meta(ticker):\n",
    "    \"\"\"Asynchronously fetches metadata for a single ticker.\"\"\"\n",
    "    t = ticker\n",
    "    try:\n",
    "        ticker_obj = yf.Ticker(t)\n",
    "        \n",
    "        # 1. Retrieve stock info\n",
    "        # Use a try/except for info specifically to avoid crashing the whole ticker\n",
    "        try:\n",
    "            info = await asyncio.to_thread(lambda: ticker_obj.info)\n",
    "        except:\n",
    "            info = {}\n",
    "            \n",
    "        sector = info.get(\"sector\")\n",
    "        industry = info.get(\"industry\")\n",
    "        mc_raw = info.get(\"marketCap\", np.nan)\n",
    "        \n",
    "        # 2. Market cap. conversion\n",
    "        if isinstance(mc_raw, (int, float)) and not pd.isna(mc_raw):\n",
    "            mc = mc_raw if t.endswith(\".TO\") else mc_raw / usd_to_cad \n",
    "        else:\n",
    "            mc = np.nan\n",
    "\n",
    "        # Define booleans safely\n",
    "        small = mc < small_cap if not pd.isna(mc) else False\n",
    "        large = mc > large_cap if not pd.isna(mc) else False\n",
    "\n",
    "        # 3. Fetch Earnings Date\n",
    "        earn = None\n",
    "        \n",
    "        # Attempt #1: Calendar (Fast)\n",
    "        try:\n",
    "            cal = await asyncio.to_thread(lambda: ticker_obj.calendar)\n",
    "            if cal and \"Earnings Date\" in cal:\n",
    "                dates = cal[\"Earnings Date\"]\n",
    "                if dates:\n",
    "                    earn = dates[0]\n",
    "        except Exception:\n",
    "            pass \n",
    "\n",
    "        # Attempt 2: Full History (Backup)\n",
    "        if earn is None:\n",
    "            try:\n",
    "                e_df = await asyncio.to_thread(lambda: ticker_obj.get_earnings_dates(limit=12))\n",
    "                if e_df is not None and not e_df.empty:\n",
    "                    future = [d.date() for d in e_df.index.to_pydatetime() if d.date() >= datetime.now().date()]\n",
    "                    if future:\n",
    "                        earn = min(future)\n",
    "            except Exception:\n",
    "                pass \n",
    "\n",
    "        return {\n",
    "            \"Ticker\": t,\n",
    "            \"Sector\": sector,\n",
    "            \"Industry\": industry,\n",
    "            \"MarketCap\": mc,\n",
    "            \"Small-cap\": small,  \n",
    "            \"Large-cap\": large,  \n",
    "            \"Earnings Date\": earn\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback in case of total code failure\n",
    "        return {\n",
    "            \"Ticker\": t,\n",
    "            \"Sector\": np.nan, \"Industry\": np.nan, \"MarketCap\": np.nan,\n",
    "            \"Small-cap\": False, \n",
    "            \"Large-cap\": False, \n",
    "            \"Earnings Date\": None\n",
    "        }\n",
    "    \n",
    "async def fetch_all_metadata(tickers):\n",
    "    print(\"Starting asynchronous metadata fetching with delay...\")\n",
    "    tasks = []\n",
    "    for i, t in enumerate(tickers):\n",
    "        # Get the metadata\n",
    "        task = async_get_meta(t)\n",
    "        tasks.append(task)\n",
    "\n",
    "        # Pause execution for a short time every few requests to reduce the risk of timeout\n",
    "        if i > 0 and i % 5 == 0: # Example: pause every 5 tickers\n",
    "            await asyncio.sleep(0.25) \n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    print(\"Metadata retrieval complete.\")\n",
    "    return results\n",
    "\n",
    "metadata_results = await fetch_all_metadata(stocks.columns)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "meta = pd.DataFrame(metadata_results).set_index(\"Ticker\")\n",
    "meta = meta.loc[stocks.columns] # Re-align index order\n",
    "meta[\"Small-cap\"] = meta[\"Small-cap\"].astype(bool)\n",
    "meta[\"Large-cap\"] = meta[\"Large-cap\"].astype(bool)\n",
    "\n",
    "# Helper functions\n",
    "def avg_volume(series):\n",
    "    \"\"\"Average daily volume after removing months with <18 trading days.\"\"\"\n",
    "    s = series.dropna()\n",
    "    if s.empty: return np.nan\n",
    "    month = s.index.to_period(\"M\")\n",
    "    valid = month.value_counts()[lambda x: x >= 18].index\n",
    "    return s[month.isin(valid)].mean()\n",
    "\n",
    "def weekly_vol(r):\n",
    "    \"\"\"Weekly volatility computed from daily returns.\"\"\"\n",
    "    w = (1 + r).resample(\"W-FRI\").prod() - 1\n",
    "    w = w.dropna()\n",
    "    return w.std() if not w.empty else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Calculate the Stock & Benchmark Metrics ===\n",
    "\n",
    "metrics = pd.DataFrame(index=stocks.columns,\n",
    "                       columns=[\"AvgVol\", \"StdDev (%)\", \"Covariance\", \"Beta\", \"Correlation\", \"WeeklyVol %\",\"IdioVol %\"],\n",
    "                       dtype=float)\n",
    "\n",
    "for t in stocks.columns:\n",
    "    r = rets[t].dropna()\n",
    "    b = bench_ret.reindex(r.index).dropna()\n",
    "\n",
    "    # Align the indexes\n",
    "    idx = r.index.intersection(b.index)\n",
    "    r = r.loc[idx]\n",
    "    b = b.loc[idx]\n",
    "\n",
    "    # Compute each of the metrics via their formulas\n",
    "    av = avg_volume(vol[t].loc[start:end])\n",
    "    sd = r.std() * 100\n",
    "    cv = r.cov(b) * (100**2)\n",
    "    beta = cv / (b.var() * (100**2)) if b.var() > 0 else np.nan\n",
    "    corr = r.corr(b)\n",
    "    wv = weekly_vol(r) * 100\n",
    "\n",
    "# --- NEW CALCULATION: Idiosyncratic Volatility ---\n",
    "# Formula: Std Dev of (Actual Return - Expected Return)\n",
    "# Expected Return = Alpha + (Beta * Benchmark Return)\n",
    "    if not np.isnan(beta):\n",
    "        # Calculate Alpha\n",
    "        alpha = r.mean() - beta * b.mean()\n",
    "        \n",
    "        # Calculate the residual value (not explained by the benchmark)\n",
    "        residuals = r - (alpha + beta * b)\n",
    "        \n",
    "        # Calculate idiosyncratic volatility (Std Dev of residuals)\n",
    "        iv = residuals.std() * 100\n",
    "    else:\n",
    "        iv = np.nan\n",
    "\n",
    "    metrics.loc[t] = [av, sd, cv, beta, corr, wv, iv]\n",
    "\n",
    "# --- Apply assignment filters & requirements ---\n",
    "\n",
    "keep_vol = metrics[\"AvgVol\"] >= min_vol\n",
    "keep_corr = metrics[\"Correlation\"] >= corr_min\n",
    "\n",
    "keep_earn = pd.Series(True, index=metrics.index)\n",
    "for t in metrics.index:\n",
    "    e = meta.loc[t, \"Earnings Date\"]\n",
    "    if meta.loc[t, \"Small-cap\"] and isinstance(e, datetime):\n",
    "        if earn_low <= e <= earn_high:\n",
    "            keep_earn[t] = False\n",
    "\n",
    "mask = keep_vol & keep_corr & keep_earn\n",
    "\n",
    "filtered = metrics[mask].join(meta, how=\"left\")\n",
    "filtered_tickers = list(filtered.index)\n",
    "\n",
    "print(\"# of Original Stocks:\", len(tickers_list))\n",
    "print(\"# of Filtered Stocks:\", len(filtered_tickers))\n",
    "display(filtered.head(len(filtered_tickers)))\n",
    "\n",
    "# --- Reference data for the benchmark ---\n",
    "\n",
    "bench_std = bench_ret.std() * 100\n",
    "bench_week = weekly_vol(bench_ret) * 100\n",
    "\n",
    "print(\"Benchmark Std Dev:\", bench_std)\n",
    "print(\"Benchmark Volatility:\", bench_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## === 3. Historical Stock Returns vs TSX/S&P 500 Benchmark ===\n",
    "\n",
    "periods = [5, 21, 63, 252]\n",
    "\n",
    "# Prices aligned with the benchmark dates\n",
    "px = stocks[filtered_tickers].loc[stocks.index.intersection(bench.index)].dropna(how=\"all\", axis=1)\n",
    "\n",
    "def trailing_ret(series, d):\n",
    "    s = series.dropna()\n",
    "    if len(s) <= 1: return np.nan\n",
    "    if len(s) <= d: return (s.iloc[-1] / s.iloc[0] - 1) * 100\n",
    "    return (s.iloc[-1] / s.iloc[-(d+1)] - 1) * 100\n",
    "\n",
    "def calc_avg_weekly(series):\n",
    "    \"\"\"Calculates the mean weekly return (%) over the available period.\"\"\"\n",
    "    # Resample to weekly (Friday close)\n",
    "    w_prices = series.dropna().resample(\"W-FRI\").last()\n",
    "    # Calculate % change\n",
    "    w_rets = w_prices.pct_change().dropna()\n",
    "    return w_rets.mean() * 100\n",
    "\n",
    "# Add \"Avg Weekly (%)\" returns to the column list\n",
    "stock_returns = pd.DataFrame(index=px.columns,\n",
    "                             columns=[f\"{d}d\" for d in periods] + [\"Avg Weekly (%)\"])\n",
    "\n",
    "for t in px.columns:\n",
    "    for d in periods:\n",
    "        stock_returns.loc[t, f\"{d}d\"] = trailing_ret(px[t], d)\n",
    "    \n",
    "    # Calculate and store the average weekly return for the stock\n",
    "    stock_returns.loc[t, \"Avg Weekly (%)\"] = calc_avg_weekly(px[t])\n",
    "\n",
    "# Returns table for the benchmark\n",
    "bench_returns = pd.DataFrame(index=[f\"{d}d\" for d in periods],\n",
    "                             columns=[\"TSX/S&P Benchmark (%)\"])\n",
    "\n",
    "for d in periods:\n",
    "    bench_returns.loc[f\"{d}d\"] = trailing_ret(bench, d)\n",
    "\n",
    "bench_returns.loc[\"Avg Weekly (%)\"] = calc_avg_weekly(bench)\n",
    "\n",
    "# Formatting (round to 2 decimal places)\n",
    "stock_returns = stock_returns.astype(float).round(2)\n",
    "bench_returns = bench_returns.astype(float).round(2)\n",
    "\n",
    "display(stock_returns)\n",
    "display(bench_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Scoring & Normalization ===\n",
    "\n",
    "# 1. Merge Dataframes\n",
    "# We need 'Avg Weekly (%)' from stock_returns combined with Beta/Idio/Cap from filtered\n",
    "df_score = filtered.join(stock_returns[\"Avg Weekly (%)\"], how=\"inner\")\n",
    "\n",
    "# 2. Get the target return for the benchmark\n",
    "target_return = bench_returns.loc[\"Avg Weekly (%)\", \"TSX/S&P Benchmark (%)\"]\n",
    "print(f\"Target Benchmark Weekly Return: {target_return}%\")\n",
    "\n",
    "# A. Beta Score (40%)\n",
    "# Logic: Minimize the difference (gap) between each Stock's Beta and 1.0\n",
    "df_score[\"Beta_Gap\"] = (df_score[\"Beta\"] - 1.0).abs()\n",
    "# Rank: Ascending = False, means the Smallest Gap gets the Highest Score\n",
    "df_score[\"Rank_Beta\"] = df_score[\"Beta_Gap\"].rank(ascending=False)\n",
    "\n",
    "# B. Return Score (20%)\n",
    "# Logic: Minimize the distance (gap) between Stock Return and Benchmark Return\n",
    "df_score[\"Return_Gap\"] = (df_score[\"Avg Weekly (%)\"] - target_return).abs()\n",
    "# Rank: Ascending = False means the Smallest Gap gets the Highest Score\n",
    "df_score[\"Rank_Return\"] = df_score[\"Return_Gap\"].rank(ascending=False)\n",
    "\n",
    "# C. Idiosyncratic Volatility Score (20%)\n",
    "# Logic: Lower is better\n",
    "# Rank: Ascending = False means the Lowest Volatility gets the Highest Score\n",
    "df_score[\"Rank_Idio\"] = df_score[\"IdioVol %\"].rank(ascending=False)\n",
    "\n",
    "# D. Market Cap Score (20%)\n",
    "# Logic: Higher is better (liquidity & index drivers)\n",
    "# Rank: Ascending = True means the Highest Cap gets the Highest Score\n",
    "df_score[\"Rank_Cap\"] = df_score[\"MarketCap\"].rank(ascending=True)\n",
    "\n",
    "# Normalize ranks to a 0-100 scale relative to the number of stocks\n",
    "n_stocks = len(df_score)\n",
    "\n",
    "# Weights: Beta = 4, Others = 2 (Total = 10 parts)\n",
    "# Formula: (4*Beta + 2*Cap + 2*Return + 2*Idio) / 10\n",
    "df_score[\"Final_Score\"] = (\n",
    "    (0.40 * df_score[\"Rank_Beta\"]) +\n",
    "    (0.20 * df_score[\"Rank_Return\"]) +\n",
    "    (0.20 * df_score[\"Rank_Idio\"]) +\n",
    "    (0.20 * df_score[\"Rank_Cap\"])\n",
    ")\n",
    "\n",
    "# Sort by the Final Score\n",
    "df_final = df_score.sort_values(\"Final_Score\", ascending=False)\n",
    "\n",
    "# --- Selection & Constraints ---\n",
    "\n",
    "# 1. Initial Attempt: Pick Top 25 Stocks\n",
    "top_25 = df_final.head(25).copy() \n",
    "\n",
    "# 2. Check if there is a Small Cap\n",
    "has_small = top_25[\"Small-cap\"].any()\n",
    "\n",
    "if not has_small:\n",
    "    #No Small Cap in Top 25. So, we swap the lowest-ranked large cap with the best-scoring small cap\n",
    "    \n",
    "    # Identify candidates: Stocks NOT in the top 25 that are small caps\n",
    "    # We use iloc[25:] to look at stocks ranked 26th and below\n",
    "    remaining_stocks = df_final.iloc[25:]\n",
    "    \n",
    "    small_cap_candidates = remaining_stocks[remaining_stocks[\"Small-cap\"] == True]\n",
    "    \n",
    "    if not small_cap_candidates.empty:\n",
    "        # A. Find the best available small cap (i.e. the first one, since df_final is sorted)\n",
    "        best_small_cap = small_cap_candidates.iloc[[0]]\n",
    "        \n",
    "        # B. Drop the lowest-ranked stock from our top 25 list (i.e. the last one)\n",
    "        top_25 = top_25.iloc[:-1]\n",
    "        \n",
    "        # C. Add the best small cap\n",
    "        top_25 = pd.concat([top_25, best_small_cap])\n",
    "        \n",
    "        print(f\"Replaced rank 25 with Small Cap '{best_small_cap.index[0]}'.\")\n",
    "\n",
    "# 3. Finalized formatting\n",
    "cols_to_show = [\"Beta\", \"Beta_Gap\", \"Avg Weekly (%)\", \"Return_Gap\", \"IdioVol %\", \"MarketCap\", \"Final_Score\", \"Small-cap\"]\n",
    "\n",
    "display(top_25[cols_to_show])\n",
    "\n",
    "print(f\"# of Stocks: {len(top_25)}\")\n",
    "print(f\"Large Cap?: {top_25['Large-cap'].any()}\")\n",
    "print(f\"Small Cap?: {top_25['Small-cap'].any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Portfolio Optimization ===\n",
    "\n",
    "def optimize_weights_for_top25(rets, stock_returns, bench_returns, top25, meta, df_score, lam=0.01):\n",
    "\n",
    "    # get list of the selected tickers\n",
    "    tickers = list(top25.index)\n",
    "\n",
    "    # get final score values for these tickers\n",
    "    score_vec = df_score.loc[tickers, \"Final_Score\"].astype(float)\n",
    "\n",
    "    # normalize scores so they sum to one\n",
    "    score_norm = score_vec / score_vec.sum()\n",
    "\n",
    "    # get each stock's weekly return\n",
    "    weekly_stock_ret = stock_returns.loc[tickers, \"Avg Weekly (%)\"].astype(float) / 100\n",
    "\n",
    "    # get benchmark weekly return\n",
    "    weekly_bench_ret = float(bench_returns.loc[\"Avg Weekly (%)\", \"TSX/S&P Benchmark (%)\"]) / 100\n",
    "\n",
    "    # objective combines strong return matching and score alignment\n",
    "    def objective(weights):\n",
    "        #compute portfolio weekly return\n",
    "        port_ret = np.dot(weights, weekly_stock_ret)\n",
    "\n",
    "        # we square the difference between portfolio return and benchmark return\n",
    "        # then multiply by 1,000,000 so that even tiny differences become extremely important to the optimizer\n",
    "        # which forecs the portfolio return to match the benchmark as close as it can\n",
    "        track_err = 1_000_000 * (port_ret - weekly_bench_ret)**2\n",
    "\n",
    "        # the score alignment encourages higher scoring stocks to have higher weights\n",
    "        score_err = np.sum((weights - score_norm)**2)\n",
    "        return track_err + lam * score_err\n",
    "\n",
    "    # weights must sum to one\n",
    "    def sum_to_one(weights):\n",
    "        return np.sum(weights) - 1\n",
    "\n",
    "    # any one sector must not exceed 40%\n",
    "    def sector_check(weights):\n",
    "        sector_totals = {}\n",
    "        for i, t in enumerate(tickers):\n",
    "            sec = meta.loc[t, \"Sector\"]\n",
    "            if sec not in sector_totals:\n",
    "                sector_totals[sec] = 0\n",
    "            sector_totals[sec] += weights[i]\n",
    "        return min(0.40 - w for w in sector_totals.values())\n",
    "\n",
    "    # ensure at least 1 large cap\n",
    "    def large_cap_check(weights):\n",
    "        total = 0\n",
    "        for i, t in enumerate(tickers):\n",
    "            if meta.loc[t, \"MarketCap\"] > 10_000_000_000:\n",
    "                total += weights[i]\n",
    "        return total - (1 / len(tickers))\n",
    "\n",
    "    # ensure at least 1 small cap\n",
    "    def small_cap_check(weights):\n",
    "        total = 0\n",
    "        for i, t in enumerate(tickers):\n",
    "            if meta.loc[t, \"MarketCap\"] < 2_000_000_000:\n",
    "                total += weights[i]\n",
    "        return total - (1 / len(tickers))\n",
    "\n",
    "    # list of constraints\n",
    "    constraints = [\n",
    "        {'type': 'eq',  'fun': sum_to_one},\n",
    "        {'type': 'ineq','fun': sector_check},\n",
    "        {'type': 'ineq','fun': large_cap_check},\n",
    "        {'type': 'ineq','fun': small_cap_check},\n",
    "    ]\n",
    "\n",
    "    # weight bounds from the assignment\n",
    "    min_weight = 1 / (2 * len(tickers))\n",
    "    bounds = [(min_weight, 0.15)] * len(tickers)\n",
    "\n",
    "    # start the near score distribution\n",
    "    starting_guess = np.array(score_norm)\n",
    "\n",
    "    # run the portfolio optimizer\n",
    "    result = minimize(\n",
    "        fun = objective,\n",
    "        x0 = starting_guess,\n",
    "        method = \"SLSQP\",\n",
    "        bounds = bounds,\n",
    "        constraints = constraints,\n",
    "        options = {\"maxiter\": 5000, \"disp\": True}\n",
    "    )\n",
    "\n",
    "    # normalize weights\n",
    "    weights = result.x\n",
    "    weights = weights / np.sum(weights)\n",
    "\n",
    "    # compute the final return and difference\n",
    "    final_port_ret = np.dot(weights, weekly_stock_ret)\n",
    "    difference = abs(final_port_ret - weekly_bench_ret)\n",
    "\n",
    "    print(\"\\nOptimization:\")\n",
    "    print(\"Benchmark Weekly Return:\", weekly_bench_ret)\n",
    "    print(\"Portfolio Weekly Return:\", final_port_ret)\n",
    "    print(\"Absolute Difference:\", difference)\n",
    "    print(\"Sum of Weights:\", np.sum(weights))\n",
    "\n",
    "    return pd.Series(weights, index=tickers)\n",
    "\n",
    "opt_weights = optimize_weights_for_top25(\n",
    "    rets = rets,\n",
    "    stock_returns = stock_returns,\n",
    "    bench_returns = bench_returns,\n",
    "    top25 = top_25,\n",
    "    meta = meta,\n",
    "    df_score = df_score\n",
    ")\n",
    "\n",
    "display(opt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Visualizing the Data ===\n",
    "\n",
    "# Temporary placeholder portfolio for Section 6 \n",
    "# (required because Portfolio_Final is created later in Section 7)\n",
    "portfolio_df = pd.DataFrame({\n",
    "    \"Ticker\": opt_weights.index,\n",
    "})\n",
    "portfolio_df[\"Value (CAD)\"] = 1.0\n",
    "portfolio_df[\"Weight (%)\"] = (1 / len(portfolio_df)) * 100\n",
    "\n",
    "# =================================\n",
    "# Correlation & Covariance Heatmaps\n",
    "# =================================\n",
    "\n",
    "# 1. Prepare data values\n",
    "heatmap_data = rets[filtered_tickers].copy()\n",
    "heatmap_data[\"Benchmark\"] = bench_ret # Add benchmark for comparison\n",
    "\n",
    "# 2. Compute the matrices\n",
    "corr_matrix = heatmap_data.corr()\n",
    "cov_matrix = heatmap_data.cov() * (100**2) # Scale up covariance for readability (like in metrics)\n",
    "\n",
    "# 3. Plot the Correlation Heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix: Filtered Stocks vs Benchmark\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explanation of Heatmap #1 ---\n",
    "print(\"\"\"The Correlation heatmap ensures diversification by revealing if stocks are redundant (moving identically) or independent.\n",
    "It is normalized on a scale of -1 to +1. By removing volatility, it tells us if stocks move in sync.\n",
    "If two stocks have a high correlation (close to 1), they are mathematically redundant. This information helps diversify a portfolio\"\"\")\n",
    "\n",
    "# 4. Plot the Covariance Heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cov_matrix, annot=True, fmt=\".2f\", cmap=\"viridis\", square=True, linewidths=0.5)\n",
    "plt.title(\"Covariance Matrix (Scaled)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explanation of Heatmap #2 ---\n",
    "print(\"\"\"The Covariance heatmap measures actual risk magnitude, identifying how volatile individual stocks are and how much they swing together.\n",
    "It combines correlation plus volatility, which tells us how much actual risk is being generated.\n",
    "A high covariance means two stocks not only move together but do so with violent swings\"\"\")\n",
    "\n",
    "# ==================================\n",
    "# Time-Series Graphs: Beta & Std Dev\n",
    "# ==================================\n",
    "\n",
    "beta_series = metrics[\"Beta\"]\n",
    "std_series = metrics[\"StdDev (%)\"]\n",
    "benchmark_beta = 1.0\n",
    "benchmark_std = bench_std\n",
    "\n",
    "# --- Plot the Beta Graph ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(beta_series.index, beta_series.values, marker='o', color='black')\n",
    "plt.axhline(y=benchmark_beta, color='red', linestyle='--', label='Benchmark Beta')\n",
    "plt.title(\"Stocks' Beta Values Relative to Benchmark\")\n",
    "plt.xlabel(\"Ticker\")\n",
    "plt.ylabel(\"Beta\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explanation of the Time-Series Graph #1 ---\n",
    "print(\"\"\"Through the beta chart, we can see a wide range of market sensitivity. While certain stocks sit well above the benchmark beta of 1.00, others fall noticeably below it. The main benefit of this graph is that it showcases the assets' differing market movements, and it helps identify positions that carry higher systematic risk versus those that behave more defensively.\n",
    "\n",
    "Because these values are spread out, the portfolio ends up holding stocks with different levels of market exposure, and, as a result, the overall risk is dispersed rather than concentrated in one category. This naturally supports diversification across both high-beta and low-beta positions.\"\"\")\n",
    "\n",
    "# --- Plot the Standard Deviation Graph ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(std_series.index, std_series.values, marker='o', color='black')\n",
    "plt.axhline(y=benchmark_std, color='red', linestyle='--', label='Benchmark Std Dev')\n",
    "plt.title(\"Average Daily Std Dev (%) by Stock\")\n",
    "plt.xlabel(\"Ticker\")\n",
    "plt.ylabel(\"Standard Deviation (%)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explanation of the Time-Series Graph #2 ---\n",
    "print(\"\"\"In the standard deviation plot, an important benefit comes from seeing how the overall volatility of each stock compares relative to the rest of the portfolio. As a result, the graph helps us identify which positions contribute to more short-term risk and which ones are more stable. \n",
    "\n",
    "Furthermore, by viewing all volatility levels together, the chart makes it easier to understand how spread out the risk is across the holdings. This allows us to judge whether the portfolio relies too heavily on a single volatility profile or whether it gains diversification by balancing varying levels of fluctuation.\"\"\")\n",
    "\n",
    "# ==========================================\n",
    "# Pie Charts: Portfolio Sectors & Weightings\n",
    "# ==========================================\n",
    "\n",
    "# uses final portfolio and meta sector info\n",
    "# temporary placeholder for Section 6 (full Portfolio_Final created in Section 7)\n",
    "portfolio_df = pd.DataFrame({\n",
    "    \"Ticker\": opt_weights.index,\n",
    "    \"Price\": stocks[opt_weights.index].iloc[-1].values,\n",
    "    \"Currency\": [\"CAD\" if t.endswith(\".TO\") else \"USD\" for t in opt_weights.index],\n",
    "})\n",
    "portfolio_df[\"Value (CAD)\"] = 1.0\n",
    "portfolio_df[\"Weight (%)\"] = (1 / len(portfolio_df)) * 100\n",
    "\n",
    "sector_series = meta[\"Sector\"]\n",
    "\n",
    "# sector breakdown for the portfolio\n",
    "sectors = portfolio_df[\"Ticker\"].map(sector_series)\n",
    "sector_totals = portfolio_df.groupby(sectors)[\"Value (CAD)\"].sum()\n",
    "\n",
    "# --- Plot the Portfolio's Sectors Chart ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sector_totals, labels=sector_totals.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Portfolio's Sector Allocations\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot the Stock Weightings Chart ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(portfolio_df[\"Weight (%)\"], labels=portfolio_df[\"Ticker\"], autopct=\"%1.1f%%\")\n",
    "plt.title(\"Stock Weighting Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# get sector totals from the final selected stocks\n",
    "benchmark_totals = portfolio_df.groupby(sectors)[\"Value (CAD)\"].sum()\n",
    "\n",
    "# convert totals to percentages for the pie chart\n",
    "benchmark_sectors = (benchmark_totals / benchmark_totals.sum()).to_dict()\n",
    "\n",
    "# --- Plot the Benchmark's Sectors Chart ---\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    list(benchmark_sectors.values()),\n",
    "    labels=list(benchmark_sectors.keys()),\n",
    "    autopct=\"%1.1f%%\"\n",
    ")\n",
    "plt.title(\"Average % per Sector in the S&P 500/TSX Benchmark\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Explanation of the Pie Charts ---\n",
    "print(\"\"\"The sector allocation chart focuses on how the portfolio’s value is spread across different industries. By looking at the distribution, we can see which areas the portfolio naturally leans toward and whether any sector carries a noticeably larger share. This makes it easier to understand where the portfolio is most exposed and where diversification is helping reduce concentration risk.\n",
    "\n",
    "The weight distribution chart focuses on the individual tickers rather than the sectors. It shows how much each position contributes to the whole portfolio, allowing us to confirm that no single stock dominates the allocation. This is especially useful for checking that the final weights line up with the optimization output. The benchmark sector mix is generated using the same set of selected stocks.\n",
    "\n",
    "Since it shows the underlying composition of the portfolio, it provides a simple reference point to compare against the sector allocation chart. Together, these visuals help clarify both the top-down (sector) and bottom-up (ticker) structure of the portfolio.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Output the Portfolio's Data Frame & Export the .csv file ===\n",
    "\n",
    "portfolio_value = 1_000_000  # CAD budget before fees\n",
    "\n",
    "tickers_sel = opt_weights.index\n",
    "prices_local = stocks[tickers_sel].iloc[-1]\n",
    "\n",
    "currency = pd.Series(\n",
    "    [\"CAD\" if t.endswith(\".TO\") else \"USD\" for t in tickers_sel],\n",
    "    index = tickers_sel\n",
    ")\n",
    "\n",
    "# Convert local prices to CAD (used for value & share calculations)\n",
    "prices_cad = pd.Series(\n",
    "    [p if currency[t] == \"CAD\" else p / usd_to_cad for t, p in prices_local.items()],\n",
    "    index = tickers_sel\n",
    ")\n",
    "\n",
    "# --- STEP #1: Allocate the full CAD budget based on stock weightings BEFORE fees ---\n",
    "pre_fee_allocation_cad = opt_weights * portfolio_value\n",
    "\n",
    "# --- STEP #2: Convert allocations into fractional shares ---\n",
    "shares = pre_fee_allocation_cad / prices_cad\n",
    "\n",
    "# --- STEP #3: Compute exact fees for each stock ---\n",
    "# Fee rule: min(2.15 USD, 0.001 USD * shares)\n",
    "fees_usd = np.minimum(2.15, 0.001 * shares)\n",
    "fees_cad = fees_usd / usd_to_cad\n",
    "total_fees_cad = fees_cad.sum()\n",
    "\n",
    "# --- STEP #4: Actual investable CAD after fees ---\n",
    "portfolio_net_cad = portfolio_value - total_fees_cad\n",
    "\n",
    "# --- STEP #5: Recalculate the final # of shares using NET capital ---\n",
    "final_allocation_cad = opt_weights * portfolio_net_cad\n",
    "final_shares = final_allocation_cad / prices_cad\n",
    "\n",
    "# --- STEP #6: Final CAD values and weights ---\n",
    "values_cad = final_shares * prices_cad\n",
    "total_value_cad = values_cad.sum()\n",
    "\n",
    "final_weights = (values_cad / total_value_cad) * 100\n",
    "\n",
    "# --- Build Portfolio_Final (index 1..n) ---\n",
    "Portfolio_Final = pd.DataFrame({\n",
    "    \"Ticker\": tickers_sel,\n",
    "    \"Price\": prices_local.values,\n",
    "    \"Currency\": currency.values,\n",
    "    \"Shares\": final_shares.values,\n",
    "    \"Value (CAD)\": values_cad.values,\n",
    "    \"Weight (%)\": final_weights.values\n",
    "})\n",
    "Portfolio_Final.index = range(1, len(Portfolio_Final) + 1)\n",
    "\n",
    "print(f\"Net Portfolio Value (CAD): ${total_value_cad:,.2f}\")\n",
    "print(f\"Total Fees Deducted (CAD): ${total_fees_cad:,.2f}\")\n",
    "print(f\"Total # of Shares: {final_shares.sum():,.2f}\")\n",
    "print(f\"Sum of Stock Weightings: {final_weights.sum():.2f}%\")\n",
    "\n",
    "display(Portfolio_Final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export CSV file (Ticker + Shares only) ---\n",
    "Stocks_Final = Portfolio_Final[[\"Ticker\", \"Shares\"]]\n",
    "Stocks_Final.to_csv(\"Stocks_Group_10.csv\", index=False, header=True)\n",
    "\n",
    "from IPython.display import FileLink, HTML\n",
    "\n",
    "print(\"Click below to download our Stocks_Final CSV:\")\n",
    "display(FileLink(\"Stocks_Group_10.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "#### Markdown Conclusion:\n",
    "According to our chosen strategy (Market Meet), our goal was to generate a portfolio return as close as possible to the benchmark average (above or below) of the S&P/TSX Composite and the S&P 500 indices.\n",
    "\n",
    "Here is a step-by-step overview of the main code sections, as well as an explanation of our understanding:\n",
    "\n",
    "#### 1. Parse the Tickers.csv file and extract ticker data (market capitalization, sector, industry, daily volume) from Yahoo Finance (yfinance).\n",
    "\n",
    "In this section, the code reads the TA-provided Tickers.csv file and downloads the year-to-date (YTD) daily price and volume data (between 2025-01-01 and 2025-11-21) for every ticker, as well as the S&P 500 and TSX Composite benchmarks. The script extracts each company’s sector, industry, market capitalization, and upcoming earnings date using yfinance.Ticker().info. Market caps are normalized into CAD, ensuring U.S. tickers (such as AAPL at approximately USD 3.0T) are directly comparable to Canadian ones under the assignment thresholds (< CAD 2B small-cap, > CAD 10B large-cap). The average daily volume is calculated after removing any month with fewer than 18 trading days. This generates a complete and consistent dataset that later filtering and calculations rely on.\n",
    "\n",
    "#### 2. Calculate metrics: Beta, Relative Standard Deviation (StdDev), Weekly Volatility (Weekly Vol), Covariance\n",
    "\n",
    "In this stage, the code computes all statistical metrics using the return series aligned to the benchmark. Daily standard deviation is converted into percentage units (e.g., many large-caps fall in the 1–3% daily volatility range, while higher-risk names may exceed 4–5%). Covariance values are scaled into percent-squared units, often ranging from 0.01 to 0.15 depending on the stock’s co-movement with the benchmark. Beta is calculated as Cov/Var, typically producing values between 0.6–1.8 depending on the stock’s sensitivity. Weekly volatility is computed by compounding daily returns into weekly returns, then taking the standard deviation of that series; stable names often show 3–6% weekly volatility, while higher-risk ones can exceed 10–12%. Correlation values usually cluster between 0.20 and 0.90.\n",
    "\n",
    "#### 3. Calculate historical stock & benchmark returns (5 days, 21d, 63d, 252d)\n",
    "\n",
    "This section computes trailing percentage returns for each stock and the benchmark across 5-day, 21-day, 63-day, and 252-day windows. When the available price history is shorter than the required window, the script uses the earliest available price to compute a valid return. These returns typically range widely: 5-day values often fall between −10% and +12% for volatile tickers, while 63-day values often lie between −20% and +25%. The benchmark’s returns are usually narrower, such as −3% to +4% over 21 days and −8% to +12% over 252 days. The resulting DataFrames present all values in percentage format for clear interpretation during subsequent scoring and selection steps.\n",
    "\n",
    "#### 4. Assign a weighted point value score per stock, for every category based on importance [Beta (40%), Market Cap (20%), Returns (20%), & Volatility (20%)]\n",
    "\n",
    "In this part of the code, each stock receives a numerical score derived from its beta, market cap, volatility, and return on investment (ROI), each normalized and then weighted by the importance levels selected by the group. The 40% beta component favours values near 1.00, the 20% market cap component increases the score of larger firms (which may range from CAD 50B to CAD 3T), and the 20% volatility component lowers the score of stocks with higher daily or weekly variability (such as weekly volatility exceeding 8–10%). The 20% returns column focuses on how accurate a stock's weekly returns are relative to the 50/50 benchmark. The sum of these points is tallied into a single total score used to rank each filtered stock.\n",
    "\n",
    "#### 5. Optimize the large-cap portfolio (# of valid filtered stocks - 1), replace the lowest scoring large-cap stock with the highest scoring small-cap stock. Note: Check if there is a small-cap in the filtered list of valid stocks.\n",
    "\n",
    "After all stocks are scored, the code selects the top-ranked large-cap stocks (market cap > CAD 10B) and checks whether any small-cap stocks (< CAD 2B) remain in the filtered set. If at least one small-cap is present, the script replaces the lowest-ranked large-cap with the highest-ranked small-cap to satisfy the assignment requirement. The optimizer then computes position weights subject to the constraints: no position above 15%, a minimum weight of 100/(2n)%, sector exposure below 40%, and total allocated amount near $1,000,000 CAD after applying the fee rule (2.15 USD or 0.001/share). Share quantities are then calculated from these weights and the last available price data.\n",
    "\n",
    "#### 6. Visualize the data (Heatmaps, Time-series graphs, Pie charts)\n",
    "\n",
    "***In this section, the code produces visual summaries that make the structure of the filtered stocks and the final portfolio easy to understand. A correlation heatmap shows how closely each stock’s returns move together and how strongly they relate to the benchmark. A covariance heatmap presents the differences in joint variability between stock pairs. Time-series plots display each stock’s beta and volatility values so they can be compared directly across the filtered universe. Sector pie charts show how the portfolio’s exposure is distributed across industries, and a separate pie chart shows the weight assigned to each stock.\n",
    "\n",
    "#### 7. Output the final stock portfolio data frame and export the Tickers + # of shares to a .csv file\n",
    "\n",
    "As the final stage, this creates the Portfolio_Final DataFrame, which contains each stock's tickers, last traded price, currency, number of shares, total CAD value, and final optimized portfolio weight. The script verifies that the portfolio value stays near the 1,000,000 CAD budget after fees and that the weights sum exactly to 100%. Note: The fee for each stock is the smaller of a $2.15 USD flat charge or 0.001 USD per share purchased; the total transaction fee (CAD) is the sum of these amounts across all stocks in the portfolio.\n",
    "\n",
    "It then exports the required Stocks_Final CSV file using only the ticker and # of shares for submission. These outputs summarize the final optimized portfolio using only the results produced by the earlier filtering, scoring, and weighting steps.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
